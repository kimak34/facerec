{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-science",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Links\n",
    "# Triplet Loss: https://www.tensorflow.org/addons/tutorials/losses_triplet\n",
    "# Triplet loss more info: https://www.tensorflow.org/addons/api_docs/python/tfa/losses/TripletSemiHardLoss\n",
    "# FaceNet: https://arxiv.org/pdf/1503.03832.pdf\n",
    "# Inception Cell: https://www.jeremyjordan.me/content/images/2018/04/Screen-Shot-2018-04-17-at-10.12.35-AM.png\n",
    "# Kaggle Dataset: https://www.kaggle.com/atulanandjha/lfwpeople\n",
    "\n",
    "# Open an example of a triple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-missile",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "golden-rochester",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-chinese",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-divide",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = r'C:\\Users\\kwens\\Documents\\Python Scripts\\TensorFlow\\Local Datasets\\face-lfw\\lfw_funneled'\n",
    "same_paths = {}\n",
    "diff_paths = {}\n",
    "\n",
    "for path_name, dir_names, file_names in os.walk(src_dir):\n",
    "    if len(file_names) > 1:\n",
    "        same_paths[path_name] = file_names\n",
    "        \n",
    "    if len(file_names) == 1:\n",
    "        diff_paths[path_name] = file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-award",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dict(path, dictionary, option):\n",
    "    for file in dictionary[path]:\n",
    "        img = cv2.imread(os.path.join(path, file))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        detected_faces = face_cascade.detectMultiScale(image=img)\n",
    "        \n",
    "        if len(detected_faces) != 1:\n",
    "            index = dictionary[path].index(file)\n",
    "            del dictionary[path][index]\n",
    "    \n",
    "    if option == 'same_paths':\n",
    "        if len(dictionary[path]) % 2 == 1:\n",
    "            del dictionary[path][-1]\n",
    "\n",
    "        if len(dictionary[path]) < 2:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    else:\n",
    "        if len(dictionary[path]) < 1:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "\n",
    "filter_func = lambda path: filter_dict(path, same_paths, 'same_paths')\n",
    "same_paths = {key: same_paths[key] for key in filter(filter_func, same_paths)}\n",
    "\n",
    "filter_func = lambda path: filter_dict(path, diff_paths, 'diff_paths')\n",
    "diff_paths = {key: diff_paths[key] for key in filter(filter_func, diff_paths)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-jimmy",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades+\"haarcascade_frontalface_default.xml\")\n",
    "tar_dir = r'C:\\Users\\kwens\\Documents\\Python Scripts\\TensorFlow\\Local Datasets\\face-lfw\\detectable\\0_null'\n",
    "\n",
    "index = 0\n",
    "\n",
    "for path in same_paths:\n",
    "    for i in range(len(same_paths[path])//2):\n",
    "        img = cv2.imread(os.path.join(path, same_paths[path][2*i]))\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        detected_faces = face_cascade.detectMultiScale(image=gray_img)\n",
    "        \n",
    "        if len(detected_faces) != 1:\n",
    "            continue\n",
    "        else:\n",
    "            offset_width, offset_height, target_width, target_height = detected_faces[0]\n",
    "            img = tf.image.crop_to_bounding_box(img, offset_height, offset_width, target_height, target_width)\n",
    "            img = tf.keras.preprocessing.image.smart_resize(img, (128, 128))\n",
    "            \n",
    "        img_1 = cv2.imread(os.path.join(path, same_paths[path][2*i+1]))\n",
    "        gray_img = cv2.cvtColor(img_1, cv2.COLOR_BGR2GRAY)\n",
    "        detected_faces = face_cascade.detectMultiScale(image=gray_img)\n",
    "        \n",
    "        if len(detected_faces) != 1:\n",
    "            continue\n",
    "        else:\n",
    "            offset_width, offset_height, target_width, target_height = detected_faces[0]\n",
    "            img_1 = tf.image.crop_to_bounding_box(img_1, offset_height, offset_width, target_height, target_width)\n",
    "            img_1 = tf.keras.preprocessing.image.smart_resize(img_1, (128, 128))\n",
    "            \n",
    "        diff_path = next(diff_iter)\n",
    "        \n",
    "        img_2 = cv2.imread(os.path.join(diff_path, diff_paths[diff_path][0]))\n",
    "        gray_img = cv2.cvtColor(img_2, cv2.COLOR_BGR2GRAY)\n",
    "        detected_faces = face_cascade.detectMultiScale(image=gray_img)\n",
    "        \n",
    "        if len(detected_faces) != 1:\n",
    "            continue\n",
    "        else:\n",
    "            offset_width, offset_height, target_width, target_height = detected_faces[0]\n",
    "            img_2 = tf.image.crop_to_bounding_box(img_2, offset_height, offset_width, target_height, target_width)\n",
    "            img_2 = tf.keras.preprocessing.image.smart_resize(img_2, (128, 128))\n",
    "        \n",
    "        tar_path = os.path.join(tar_dir, str(index))\n",
    "        os.mkdir(tar_path)\n",
    "        \n",
    "        cv2.imwrite(os.path.join(tar_path, same_paths[path][2*i]), img.numpy())\n",
    "        cv2.imwrite(os.path.join(tar_path, same_paths[path][2*i+1]), img_1.numpy())\n",
    "        cv2.imwrite(os.path.join(tar_path, diff_paths[diff_path][0]), img_2.numpy())\n",
    "        \n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-membership",
   "metadata": {},
   "source": [
    "# Form tf.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-problem",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\kwens\\Documents\\Python Scripts\\TensorFlow\\Local Datasets\\face-lfw\\detectable\\0_null'\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    path,\n",
    "    shuffle=False,\n",
    "    batch_size=3,\n",
    "    image_size=(128, 128)\n",
    ") # len(train_ds) --> 2845"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-wales",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def to_triplets(images, labels):\n",
    "    anchor, positive, negative = images\n",
    "\n",
    "    anchor = tf.image.rgb_to_grayscale(anchor/255.)\n",
    "    positive = tf.image.rgb_to_grayscale(positive/255.)\n",
    "    negative = tf.image.rgb_to_grayscale(negative/255.)\n",
    "    \n",
    "    return anchor, positive, negative\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_ds = train_ds.map(to_triplets)\n",
    "train_ds = train_ds.shuffle(1000)\n",
    "train_ds = train_ds.batch(batch_size)\n",
    "train_ds = train_ds.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-memphis",
   "metadata": {},
   "source": [
    "# Define Model and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-package",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Concatenate, Conv2D, Dense, Dropout, GlobalAveragePooling2D, MaxPool2D\n",
    "\n",
    "# Modified GoogLeNet or Inception v1\n",
    "#\n",
    "# \"Going Deeper with Convolutions\"\n",
    "# Christian Szegedy et al.\n",
    "#\n",
    "# https://arxiv.org/pdf/1409.4842.pdf\n",
    "\n",
    "class InceptionCell(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_dim):\n",
    "        super().__init__(self)\n",
    "        \n",
    "        self.reduce_11 = Conv2D( output_dim//4, (1, 1), strides=(1, 1), activation='relu' )\n",
    "\n",
    "        self.reduce_12 = Conv2D( (output_dim*3)//8, (1, 1), strides=(1, 1), activation='relu' )\n",
    "        self.conv_12 = Conv2D( output_dim//2, (3, 3), strides=(1, 1), activation='relu', padding='same' )\n",
    "\n",
    "        self.reduce_13 = Conv2D( output_dim//16, (1, 1), strides=(1, 1), activation='relu' )\n",
    "        self.conv_13 = Conv2D( output_dim//8, (5, 5), strides=(1, 1), activation='relu', padding='same' )\n",
    "\n",
    "        self.maxpool_14 = MaxPool2D( (3, 3), strides=(1, 1), padding='same' )\n",
    "        self.reduce_14 = Conv2D( output_dim//8, (1, 1), strides=(1, 1), activation='relu' )\n",
    "\n",
    "        self.concat_1 = Concatenate()\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        \n",
    "        x_11 = self.reduce_11(x)\n",
    "        x_12 = self.reduce_12(x); x_12 = self.conv_12(x_12)\n",
    "        x_13 = self.reduce_13(x); x_13 = self.conv_13(x_13)\n",
    "        x_14 = self.maxpool_14(x); x_14 = self.reduce_14(x_14)\n",
    "\n",
    "        x = self.concat_1([x_11, x_12, x_13, x_14])\n",
    "\n",
    "        return x\n",
    "\n",
    "# Modified GoogLeNet or Inception v1\n",
    "#\n",
    "# \"Going Deeper with Convolutions\"\n",
    "# Christian Szegedy et al.\n",
    "#\n",
    "# https://arxiv.org/pdf/1409.4842.pdf\n",
    "\n",
    "class GoogLeNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__(self)\n",
    "        \n",
    "        # Stem \n",
    "        # Input shape: (None, 128, 128, 3)\n",
    "        self.conv_s1 = Conv2D(64, (7, 7), strides=(2, 2), activation='relu', padding='same')  # (None, 64, 64, 64)\n",
    "        self.maxpool_s1 = MaxPool2D((3, 3), strides=(2, 2), padding='same')                   # (None, 32, 32, 64)\n",
    "        self.conv_s2 = Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='same') # (None, 32, 32, 128)\n",
    "        self.maxpool_s2 = MaxPool2D((3, 3), strides=(2, 2), padding='same')                   # (None, 16, 16, 128)\n",
    "\n",
    "        # Inception Cell 1\n",
    "        self.icell_1 = InceptionCell(256)                                    # (None, 16, 16, 256)\n",
    "\n",
    "        # Inception Cell 2\n",
    "        self.icell_2 = InceptionCell(416)                                    # (None, 16, 16, 416)\n",
    "\n",
    "        # MaxPooling 1\n",
    "        self.maxreduce_1 = MaxPool2D((3, 3), strides=(2, 2), padding='same') # (None, 8, 8, 416)\n",
    "\n",
    "        # Inception Cell 3\n",
    "        self.icell_3 = InceptionCell(512)                                    # (None, 8, 8, 512)\n",
    "\n",
    "        # Inception Cell 4\n",
    "        self.icell_4 = InceptionCell(512)                                    # (None, 8, 8, 512)\n",
    "\n",
    "        # Inception Cell 5\n",
    "        self.icell_5 = InceptionCell(832)                                    # (None, 8, 8, 832)\n",
    "\n",
    "        # MaxPooling 2\n",
    "        self.maxreduce_2 = MaxPool2D((3, 3), strides=(2, 2), padding='same') # (None, 4, 4, 832)\n",
    "\n",
    "        # Inception Cell 6\n",
    "        self.icell_6 = InceptionCell(1024)                                   # (None, 4, 4, 1024)\n",
    "\n",
    "        # Inception Cell 7\n",
    "        self.icell_7 = InceptionCell(1024)                                   # (None, 4, 4, 1024)\n",
    "\n",
    "        # GlobalAverage\n",
    "        self.globalav = GlobalAveragePooling2D()                             # (None, 1024)\n",
    "\n",
    "        # Head\n",
    "        self.dropout = Dropout(0.3)\n",
    "        self.dense_h1 = Dense(1024, activation='relu')                       # (None, 1024)\n",
    "        self.dense_h2 = Dense(128)                                           # (None, 128)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training=True):\n",
    "        x = inputs\n",
    "\n",
    "        x = self.conv_s1(x)\n",
    "        x = self.maxpool_s1(x)\n",
    "        x = self.conv_s2(x)\n",
    "        x = self.maxpool_s2(x)\n",
    "\n",
    "        x = self.icell_1(x)\n",
    "        x = self.icell_2(x)\n",
    "\n",
    "        x = self.maxreduce_1(x)\n",
    "\n",
    "        x = self.icell_3(x)\n",
    "        x = self.icell_4(x)\n",
    "        x = self.icell_5(x)\n",
    "\n",
    "        x = self.maxreduce_2(x)\n",
    "\n",
    "        x = self.icell_6(x)\n",
    "        x = self.icell_7(x)\n",
    "\n",
    "        x = self.globalav(x)\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.dense_h1(x)\n",
    "        x = self.dense_h2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# \"FaceNet: A Unified Embedding for Face Recognition and Clustering\"\n",
    "# Florian Schroff, Dmitry Kalenichenko, and James Philbin\n",
    "#\n",
    "# https://arxiv.org/pdf/1503.03832.pdf\n",
    "\n",
    "class TripletLoss():\n",
    "    @tf.function\n",
    "    def __call__(self, inputs, alpha):\n",
    "        anchor, positive, negative = inputs\n",
    "        \n",
    "        anchor = tf.math.l2_normalize(anchor, axis=-1)\n",
    "        positive = tf.math.l2_normalize(positive, axis=-1)\n",
    "        negative = tf.math.l2_normalize(negative, axis=-1)\n",
    "\n",
    "        anchor_positive = tf.math.reduce_sum((anchor-positive)**2, -1)\n",
    "        anchor_negative = tf.math.reduce_sum((anchor-negative)**2, -1)\n",
    "\n",
    "        loss = anchor_positive-anchor_negative+alpha\n",
    "        loss = tf.math.maximum(loss, 0.)\n",
    "\n",
    "        return tf.math.reduce_mean(loss, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-hanging",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-attraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = GoogLeNet()\n",
    "\n",
    "loss_fn = TripletLoss()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.05, momentum=0.9)\n",
    "epochs = 25\n",
    "\n",
    "@tf.function\n",
    "def train_step(batch):\n",
    "    anchor, positive, negative = batch\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        anchor = embedder(anchor, training=True)\n",
    "        positive = embedder(positive, training=True)\n",
    "        negative = embedder(negative, training=True)\n",
    "        \n",
    "        loss = loss_fn((anchor, positive, negative), 0.2)\n",
    "    \n",
    "    grads = tape.gradient(loss, embedder.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, embedder.trainable_weights))\n",
    "\n",
    "    return loss\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch {} Begin.\\n'.format(epoch))\n",
    "\n",
    "    for step, batch in enumerate(train_ds):\n",
    "        loss = train_step(batch)\n",
    "\n",
    "        if (step+1) % 10 == 0:\n",
    "            print('Step {} Complete.'.format(step))\n",
    "\n",
    "    print('\\nEpoch {} (Step {}) Training Loss: {}'.format(epoch, step, loss))\n",
    "\n",
    "    print('\\nEpoch {} Complete.'.format(epoch))\n",
    "    print('-'*100, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-guinea",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = r'C:\\Users\\kwens\\Documents\\Python Scripts\\TensorFlow\\Archive\\State of My Art\\weights_v2.h5'\n",
    "embedder.save_weights(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-asthma",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "center-vegetarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Concatenate, Conv2D, Dense, Dropout, GlobalAveragePooling2D, MaxPool2D\n",
    "\n",
    "# Modified GoogLeNet or Inception v1\n",
    "#\n",
    "# \"Going Deeper with Convolutions\"\n",
    "# Christian Szegedy et al.\n",
    "#\n",
    "# https://arxiv.org/pdf/1409.4842.pdf\n",
    "\n",
    "class InceptionCell(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_dim):\n",
    "        super().__init__(self)\n",
    "        \n",
    "        self.reduce_11 = Conv2D( output_dim//4, (1, 1), strides=(1, 1), activation='relu' )\n",
    "\n",
    "        self.reduce_12 = Conv2D( (output_dim*3)//8, (1, 1), strides=(1, 1), activation='relu' )\n",
    "        self.conv_12 = Conv2D( output_dim//2, (3, 3), strides=(1, 1), activation='relu', padding='same' )\n",
    "\n",
    "        self.reduce_13 = Conv2D( output_dim//16, (1, 1), strides=(1, 1), activation='relu' )\n",
    "        self.conv_13 = Conv2D( output_dim//8, (5, 5), strides=(1, 1), activation='relu', padding='same' )\n",
    "\n",
    "        self.maxpool_14 = MaxPool2D( (3, 3), strides=(1, 1), padding='same' )\n",
    "        self.reduce_14 = Conv2D( output_dim//8, (1, 1), strides=(1, 1), activation='relu' )\n",
    "\n",
    "        self.concat_1 = Concatenate()\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        \n",
    "        x_11 = self.reduce_11(x)\n",
    "        x_12 = self.reduce_12(x); x_12 = self.conv_12(x_12)\n",
    "        x_13 = self.reduce_13(x); x_13 = self.conv_13(x_13)\n",
    "        x_14 = self.maxpool_14(x); x_14 = self.reduce_14(x_14)\n",
    "\n",
    "        x = self.concat_1([x_11, x_12, x_13, x_14])\n",
    "\n",
    "        return x\n",
    "\n",
    "# Modified GoogLeNet or Inception v1\n",
    "#\n",
    "# \"Going Deeper with Convolutions\"\n",
    "# Christian Szegedy et al.\n",
    "#\n",
    "# https://arxiv.org/pdf/1409.4842.pdf\n",
    "\n",
    "class GoogLeNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__(self)\n",
    "        \n",
    "        # Stem \n",
    "        # Input shape: (None, 128, 128, 3)\n",
    "        self.conv_s1 = Conv2D(64, (7, 7), strides=(2, 2), activation='relu', padding='same')  # (None, 64, 64, 64)\n",
    "        self.maxpool_s1 = MaxPool2D((3, 3), strides=(2, 2), padding='same')                   # (None, 32, 32, 64)\n",
    "        self.conv_s2 = Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='same') # (None, 32, 32, 128)\n",
    "        self.maxpool_s2 = MaxPool2D((3, 3), strides=(2, 2), padding='same')                   # (None, 16, 16, 128)\n",
    "\n",
    "        # Inception Cell 1\n",
    "        self.icell_1 = InceptionCell(256)                                    # (None, 16, 16, 256)\n",
    "\n",
    "        # Inception Cell 2\n",
    "        self.icell_2 = InceptionCell(416)                                    # (None, 16, 16, 416)\n",
    "\n",
    "        # MaxPooling 1\n",
    "        self.maxreduce_1 = MaxPool2D((3, 3), strides=(2, 2), padding='same') # (None, 8, 8, 416)\n",
    "\n",
    "        # Inception Cell 3\n",
    "        self.icell_3 = InceptionCell(512)                                    # (None, 8, 8, 512)\n",
    "\n",
    "        # Inception Cell 4\n",
    "        self.icell_4 = InceptionCell(512)                                    # (None, 8, 8, 512)\n",
    "\n",
    "        # Inception Cell 5\n",
    "        self.icell_5 = InceptionCell(832)                                    # (None, 8, 8, 832)\n",
    "\n",
    "        # MaxPooling 2\n",
    "        self.maxreduce_2 = MaxPool2D((3, 3), strides=(2, 2), padding='same') # (None, 4, 4, 832)\n",
    "\n",
    "        # Inception Cell 6\n",
    "        self.icell_6 = InceptionCell(1024)                                   # (None, 4, 4, 1024)\n",
    "\n",
    "        # Inception Cell 7\n",
    "        self.icell_7 = InceptionCell(1024)                                   # (None, 4, 4, 1024)\n",
    "\n",
    "        # GlobalAverage\n",
    "        self.globalav = GlobalAveragePooling2D()                             # (None, 1024)\n",
    "\n",
    "        # Head\n",
    "        self.dropout = Dropout(0.3)\n",
    "        self.dense_h1 = Dense(1024, activation='relu')                       # (None, 1024)\n",
    "        self.dense_h2 = Dense(128)                                           # (None, 128)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training=True):\n",
    "        x = inputs\n",
    "\n",
    "        x = self.conv_s1(x)\n",
    "        x = self.maxpool_s1(x)\n",
    "        x = self.conv_s2(x)\n",
    "        x = self.maxpool_s2(x)\n",
    "\n",
    "        x = self.icell_1(x)\n",
    "        x = self.icell_2(x)\n",
    "\n",
    "        x = self.maxreduce_1(x)\n",
    "\n",
    "        x = self.icell_3(x)\n",
    "        x = self.icell_4(x)\n",
    "        x = self.icell_5(x)\n",
    "\n",
    "        x = self.maxreduce_2(x)\n",
    "\n",
    "        x = self.icell_6(x)\n",
    "        x = self.icell_7(x)\n",
    "\n",
    "        x = self.globalav(x)\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.dense_h1(x)\n",
    "        x = self.dense_h2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "motivated-british",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = GoogLeNet()\n",
    "embedder(np.zeros((1, 128, 128, 1)))\n",
    "\n",
    "load_path = r'C:\\Users\\kwens\\Documents\\Python Scripts\\TensorFlow\\Archive\\State of My Art\\weights_v2.h5'\n",
    "embedder.load_weights(load_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-pound",
   "metadata": {},
   "source": [
    "# Build Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "wanted-creek",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclid_dist(v1, v2):\n",
    "    v1 = tf.math.l2_normalize(v1, -1)\n",
    "    v2 = tf.math.l2_normalize(v2, -1)\n",
    "    \n",
    "    dist = tf.math.reduce_sum((v1-v2)**2, -1)\n",
    "    dist = tf.math.sqrt(dist)\n",
    "\n",
    "    return dist\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    cut = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    detected_faces = face_cascade.detectMultiScale(image=cut)\n",
    "\n",
    "    if len(detected_faces) != 1:\n",
    "        return True, None, None, None, None\n",
    "\n",
    "    offset_width, offset_height, target_width, target_height = detected_faces[0]\n",
    "\n",
    "    cut = tf.image.crop_to_bounding_box(\n",
    "        tf.reshape(cut, (480, 640, 1)), \n",
    "        offset_height, offset_width, \n",
    "        target_height, target_width\n",
    "    )\n",
    "    cut = tf.keras.preprocessing.image.smart_resize(cut, (128, 128))\n",
    "    cut = tf.reshape(cut, (1, 128, 128, 1))\n",
    "    \n",
    "    return cut, offset_width, offset_height, target_width, target_height\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades+\"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "features_list = []\n",
    "key = None\n",
    "proceed = False\n",
    "skip = False\n",
    "\n",
    "#test = []\n",
    "\n",
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "while(True):\n",
    "    if key == ord('q'): # Quit\n",
    "        break\n",
    "    \n",
    "    if key == ord('a'): # Anchor\n",
    "        if skip:\n",
    "            _, frame = cam.read()\n",
    "            \n",
    "            cv2.imshow('', frame)\n",
    "            \n",
    "            key = cv2.waitKey(10)\n",
    "            continue\n",
    "        \n",
    "        for i in range(100):\n",
    "            _, frame = cam.read()\n",
    "            \n",
    "            cut, offset_width, offset_height, target_width, target_height = preprocess_frame(frame)\n",
    "            \n",
    "            if cut is True:\n",
    "                _, frame = cam.read()\n",
    "\n",
    "                cv2.imshow('', frame)\n",
    "\n",
    "                key = cv2.waitKey(10)\n",
    "                continue\n",
    "            \n",
    "            features_list.append(embedder(cut, training=False))\n",
    "            \n",
    "            color = (255, 255, 0)\n",
    "\n",
    "            frame = cv2.rectangle(\n",
    "                frame, (offset_width, offset_height), \n",
    "                (offset_width+target_width, offset_height+target_height), \n",
    "                color, 1\n",
    "            )\n",
    "\n",
    "            cv2.imshow('', frame)\n",
    "            \n",
    "            key = cv2.waitKey(10)\n",
    "        \n",
    "        features = np.mean(features_list, axis=0)\n",
    "        \n",
    "        proceed = True\n",
    "        skip = True\n",
    "        \n",
    "        continue\n",
    "    \n",
    "    if proceed:\n",
    "        _, frame = cam.read()\n",
    "\n",
    "        cut, offset_width, offset_height, target_width, target_height = preprocess_frame(frame)\n",
    "\n",
    "        if cut is True:\n",
    "            _, frame = cam.read()\n",
    "\n",
    "            cv2.imshow('', frame)\n",
    "\n",
    "            key = cv2.waitKey(10)\n",
    "            continue\n",
    "\n",
    "        current_features = embedder(cut, training=False)\n",
    "        \n",
    "        #print(euclid_dist(features, current_features))\n",
    "        #test.append(euclid_dist(features, current_features))\n",
    "\n",
    "        if euclid_dist(features, current_features) <= 0.004:\n",
    "            # For same faces:      d(f1, f2) approx. 0.0027\n",
    "            # For different faces: d(f1, f2) approx. 0.005\n",
    "            color = (0, 255, 0)\n",
    "        else:\n",
    "            color = (0, 0, 255)\n",
    "\n",
    "        frame = cv2.rectangle(\n",
    "            frame, (offset_width, offset_height), \n",
    "            (offset_width+target_width, offset_height+target_height), \n",
    "            color, 1\n",
    "        )\n",
    "\n",
    "        cv2.imshow('', frame)\n",
    "\n",
    "        key = cv2.waitKey(10)\n",
    "\n",
    "    else:\n",
    "        _, frame = cam.read()\n",
    "\n",
    "        cut = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        detected_faces = face_cascade.detectMultiScale(image=cut)\n",
    "\n",
    "        if len(detected_faces) != 1:\n",
    "            _, frame = cam.read()\n",
    "\n",
    "            cv2.imshow('', frame)\n",
    "\n",
    "            key = cv2.waitKey(10)\n",
    "            continue\n",
    "\n",
    "        offset_width, offset_height, target_width, target_height = detected_faces[0]\n",
    "        \n",
    "        color = (255, 0, 0)\n",
    "        \n",
    "        frame = cv2.rectangle(\n",
    "            frame, (offset_width, offset_height), \n",
    "            (offset_width+target_width, offset_height+target_height), \n",
    "            color, 1\n",
    "        )\n",
    "        \n",
    "        cv2.imshow('', frame)\n",
    "        \n",
    "        key = cv2.waitKey(10)\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-raising",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
